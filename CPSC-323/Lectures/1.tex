\section{August 29, 2018}

The beginning of this lecture was mostly administrata.
Stan discussed the structure of the course and the various grading policies.
Of particular importance are the facts that, unlike in CPSC 223, the public test cases for our assignments will only cover $\sim 25\%$ of the private ones.
If you read between the lines of the public test cases, you can probably guess another $25\%$ of the private tests, and a careful reading of the entire program specification will net you almost all of the private test cases, so long as your logic holds.
It's also important to note that 323 has a ``line allowance'' for code, usually around 180 lines (after all the comments and lone braces have been remove).
This means that if your code is $300+$ lines, it might be best to simply scrap the assignment and start from scratch since you probably aren't implementing it in the correct or most efficient way.

\subsection*{(Binary) Number Representation}

Stan began the actual academic part of the lecture by discussing how arithmetic works on an arbitrary binary machine.
Because this is computer science and not mathematics, we are often constrained by limits that do not constrain mathematicians.
Notably, we have to worry about whether or not a number is representable by our machine.

\begin{definition}[Representable Numbers]
The \emph{representable numbers} is the set of all possible bit patters for a given level of precision.
An element of this set is said to be representable since it can be stored in a computer's memory and can be operated upon.
Since we are discussing classical computers in this course, the representable numbers are a proper subset of $\R$.
In practice, representable numbers are either integral (and so they are a proper subset of $\Z$) or they are floating-point numbers with a fixed degree of precision, and so there exists an injective map between these representable numbers and $\Z$.
\end{definition}


Unlike in mathematics, representable numbers are not closed under the standard arithmetic operations, and so the result of arithmetic expressions is not guaranteed to be representable.
In practice, programmers give special names to the ways in which the representable numebers fail to be closed under these operations:

\begin{itemize}
	\item Overflow: When $a \star b$ is too large to be represented. Consider $\texttt{INT\_MAX} + 1$;
	\item Underflow: When $a \star b$ is too small to be represented. Consider $1 / \num{e30}$, which is approximately zero but definitely not equal to zero. 
	\item Inexactness: When $a \star b$ requires too much precision. Consider $1/3 = 0.33333333\dots$, which na√Øvely requires an infinite precision to represent accurately.
\end{itemize}

This leads to expressions which break the traditional axioms of field mathematics.
Namely,

\begin{itemize}
\item Operations need not be associative. Consider that if $a < 0$ and $b + c > \texttt{INT\_MAX}$, then $a + (b+c) \not= (a + b) + c$;
\item Operations need not be distributive. Compilers can expand code in mysterious ways and this can bite you, especially with floating point multiplication;
\item It's not guaranteed that $x < y \iff x-y < 0$, since $x-y$ may not be representable.
\end{itemize}

Incongruency between the worlds of Math and CS leads to losses in the billions of dollars.
The moral of the story is that it's important and not trivial to check that our software works the way we think it ought to.
Never assume that machine arithmetic is correct.

\subsubsection*{Nonnegative Numbers}

Consider a number $N$, represented with $m$ bits of precision by
\[ N = \beta_{m-1}\beta_{m-2}\cdots\beta_2\beta_1\beta_0 \equiv \sum_{i=0}^{m-1} \beta_i \cdot 2^i, \]
where $\beta_i$ are the place value digits of $N$.

If $\beta_0$ is on the right, we say the representation is \emph{Little Endian}. Otherwise, we say that it is \emph{Big Endian}. Notice that if $N \geq 2^m$, then $N$ is not representable since it is too big.

\subsubsection*{Nonnegative Numbers in the C Standard}

For the scope of this discussion, we'll assume that all types today are \emph{unsigned}, so that $1 + \texttt{INT\_MAX} = 0$.
Here are the several integral types in C, along with their size on the Zoo\note{The Zoo is Yale's network of UNIX machines used by the CS department} machines and their specified size in the C standard.

\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
Type & Zoo Size & C Standard Size \\
\midrule
\texttt{char} & 8 bits & $\geq 8$ bits \\
\texttt{short} & 16 bits & $\geq 16$ bits \\
\texttt{int} & 32 bits & $\geq 16$ bits \\
\texttt{long} & 64 bits & $\geq 32$ bits \\
\texttt{long long} & 64 bits & $\geq 64$ bits \\
\bottomrule
\end{tabular}
\end{center}

C also requires that each of these be less than or equal to the one succeding it, so
\[ \texttt{char} \leq \texttt{short} \leq \texttt{int} \leq \texttt{long} \leq \texttt{long long}. \]
If you want to write portable code, never violate these limits regardless of your machine's implementation.
There is a hierarchy of how portable your code is.
\begin{itemize}
\item Portable: Will run the same on anything.
\item Transportable: Use \texttt{\#define/typedefs} to mix your code around based on the machine. 
You can also include \texttt{limits.h} and \texttt{stdint.h} which define custom types with specific bit lengths, like \texttt{int\_64} for an integer which is guaranteed to have 64 bits of precision.
\end{itemize}
Ideally, your code will be portable, but that can be very difficult since there are some \emph{odd} machines out there which will fuck with your program and with common sense in general.

\subsection*{Operaitons on Nonnegative Numbers}

Some quick key highlights about operations on representable numbers:
\begin{itemize}
\item Subtraction leads to negatice numbers very quickly
\item Multiplicaiton is repeated addition and bit shifting if you don't care about efficiency. Otherwise, there are more advanced multiplication algorithms.
\item Division is repeated subtraction and shifting, so it caries the same caveats as multiplication.
\item Therefore, we'll focus mostly on addition.
\end{itemize}

\subsubsection*{Addition of Nonnegative Numbers}

Addition is based on a type of circuit known as a \emph{Full Adder}, which we'll treat as a black box. There are two value inputs, a carry input, a carry output, and a sum output.
Each input and output is one bit in size.

\begin{figure}[h]
\begin{center}
\begin{circuitikz}
	\draw (0,0) node[fadder] (F1) {Adder};
	\draw (F1.in1) --++ (0.0, 0.5) node[anchor=south] {$a$};
	\draw (F1.in2) --++ (0.0, 0.5) node[anchor=south] {$b$};
	\draw (F1.sum) --++ (0.0, -0.5) node[anchor=north] {sum};
	\draw (F1.cin) --++ (-0.5, 0.0) node[right = 0.5cm] {carry in};
	\draw (F1.cout) --++ (0.5, 0.0) node[left = 0.5cm] {carry out};
\end{circuitikz}
\end{center}
\end{figure}

Here, the ``carry in'' bit tells us whether or not the previous sum resulted in a magnitude large enough that we must ``carry'' a digit over to the current sum.
Similarly, the ``carry out'' bit tells the next calculation whether the current sum was too large to fit in a single bit.
The bits $a$ and $b$ are the input bits of the two numbers we're adding, and ``sum'' is their sum.
Mathematically, 
\[ \textrm{sum} = a \oplus b \oplus \textrm{carry in}. \]
In a half adder, carry in is assumed to be zero.
To add two $m$ bit numbers, we use a \emph{Ripple Carry Adder}, which is just $m$ full adders daisy-chained together, connecting the ``cary out'' bit of the previous sum to the ``carry in'' bit of the next.
\begin{figure}[h]
\begin{center}
\begin{circuitikz}
\draw (0,0) node[fadder] (F1) {$\beta_{m-1}$};
\draw (2.5,0) node[fadder] (F2) {$\beta_{m-2}$};

\draw (5,0) node[fadder] (F3) {$\beta_1$};
\draw (7.5,0) node[fadder] (F4) {$\beta_0$};

% Connect the carries
\draw (F1.cin) -- (F2.cout);
\draw[dashed] (F2.cin) -- (F3.cout);
\draw (F3.cin) -- (F4.cout);
\draw (F4.cin) --++ (-0.5, 0.0) node[right = 0.5cm] {$0$};
\draw (F1.cout) --++ (0.5, 0.0) node[left = 0.5cm] {overflow};

% Draw the input/sums
\draw (F1.in1) --++ (0.0, 0.2); 
\draw (F1.in2) --++ (0.0, 0.2);
\draw (F1.sum) --++ (0.0, -0.2); 

\draw (F2.in1) --++ (0.0, 0.2); 
\draw (F2.in2) --++ (0.0, 0.2);
\draw (F2.sum) --++ (0.0, -0.2); 

\draw (F3.in1) --++ (0.0, 0.2); 
\draw (F3.in2) --++ (0.0, 0.2);
\draw (F3.sum) --++ (0.0, -0.2); 

\draw (F4.in1) --++ (0.0, 0.2); 
\draw (F4.in2) --++ (0.0, 0.2);
\draw (F4.sum) --++ (0.0, -0.2); 
\end{circuitikz}
\end{center}
\end{figure}
This is a very simplistic way to add numbers --- it's exactly the algorithm learned in grade school for adding two numbers together.
The final carry out bit determines whether or not the number will overflow --- that is, is the number representable by our machine.
Unfortinately, it's not particularly efficient; 
The time and space complexity of the ripple carry algorithm is $\mathcal{O}(m)$.

\subsubsection*{Fast Adders}

Of course, we could do the whole process with only one adder if we store the intermediate results, but that doesn't get us any improvement in the time or space complexity.
A change in thinking is needed if we want to find a faster method of adding.
If we think about it for a bit, we realize that the slow part of the algorithm isn't the part which adds the bits together; rather, it's getting each carry sent off to the next circuit.
If we could somehow know the carries ahead of time, the entire problem could be done in parallel in constant time.

% Consider the simplistic divide-and-conquer approach:
% \[ a = (a_L , a_R) \qquad b = (b_L , b_R) \qquad s = (s_L , S_r) \qquad co. \]

% We compute $a_R + b_R \equiv co_R : s_R$, then $a_L + b_L + 0 \equiv$ and $a_L + b_L + 1$. This is 3 $m/2$ bit additions in parallel. Then $a +b = co : s$.

% If $co_R = 0$, then $ \cdots + 0$ is the correct value, otherwise $\cdots + 1$ is the correct value.

In some cases, we can determine the ``carry out'' of a given digit without considering what the ``carry in'' bit is.
For example, if $a_i = b_i = 1$, than $\text{``carry out''}_i = 1$ regardless of the ``carry in''.
We can use this to build a divide-and-conquer approach which has significant time advantages over the ripple carry adder, but pays a heavy price in terms of space.
The time for this is $\mathrm{Time}(m/2) + 1 = \mathcal{O}(\log_2 m)$, but the space is $3\cdot \mathrm{Space}(m/2) + \mathcal{O}(m) = \mathcal{O}(m^{\log_2 3})$.
Since $\log_2 3 > 1$, the space complexity grows superlinearly in $m$.